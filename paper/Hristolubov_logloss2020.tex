%% ============================================
%% ================ Preambule =================
%% ============================================
\documentclass[]{scrartcl}
\usepackage[margin = 0.5in]{geometry}

\usepackage[pdftex,unicode, 
colorlinks=true,
linkcolor = blue]{hyperref}	% нумерование страниц, ссылки!!!!ИМЕННО В ТАКОМ ПОРЯДКЕ СО СЛЕДУЮЩИМ ПАКЕТОМ
%\usepackage[warn]{mathtext}				% Поддержка русского текста в формулах
\usepackage[T1, T2A]{fontenc}			% Пакет выбора кодировки и шрифтов
\usepackage[utf8]{inputenc} 			% любая желаемая кодировка
\usepackage[english]{babel}		% поддержка русского языка
\usepackage{wrapfig}					% Плавающие картинки
\usepackage{amssymb, amsmath}			% стилевой пакет для формул
\usepackage{algorithm}
\usepackage{algorithmic} 


\ifpdf
\usepackage{cmap} 				% чтобы работал поиск по PDF
\usepackage[pdftex]{graphicx}
%\usepackage{pgfplotstable}		% Для вставки таблиц.
\pdfcompresslevel=9 			% сжимать PDF
\else
\usepackage{graphicx}
\fi

\graphicspath{{./figures/}}
\usepackage{subcaption}
%% ============================================
%% ================ Info =================
%% ============================================
\title{Loss function optimization in the click prediction models}
\author{\begin{tabular}{c c}
	  	 Maxim Hristolubov & Daniil Merkulov \\
		 \texttt{khristolyubov.me@phystech.edu} &\texttt{daniil.merkulov@skoltech.ru} 
		\end{tabular}}
\date{Project Proposal}

\begin{document}

\maketitle

\begin{abstract}

Решается задача оптимизации логистической функции потерь с квадратичной регуляризацией, возникающей в задаче предсказания кликов пользователей. Особенностью задачи является факт наличия у данных двух типов признаков: плотных (сотни) и разряженных (десятки миллионов), для каждой группы свой коэффициент регуляризации. Предполагается использовать ускоренные адаптивные методы стохастического градиентного спуска.
\end{abstract}

\section{Problem}

Необходимо решить задачу минимазации:

$$F(w)=\frac{1}{m}\sum\limits_{k=1}^{m}f_k(\boldsymbol w,(\boldsymbol x_k, y_k))+g(w),$$

$$f_k(\boldsymbol w,(\boldsymbol x_k, y_k)) = ln(1+\exp(-y_k\boldsymbol w^T \boldsymbol x_k)),$$

$$g(w)=\lambda_1\sum\limits_{i=1}^{n_1}w_i^2 + \lambda_2\sum\limits_{i=n_1+1}^{n_1+n_2}w_i^2,$$

where $\boldsymbol y\in\{1,-1\}^m$, $\boldsymbol x_k, \boldsymbol w\in R^n$, $\forall 1\leq i\leq n_1:x_{ki} \neq 0$ for almost all $k\leq m$, and $\forall n_1+1\leq i\leq n_1+n_2:x_{ki} = 0$ for many $k\leq m$.

В связи с тем, что целевая функция представлена большим количеством слагаемых целесообразно начать решение со стохастического градиентного спуска, то есть брать градиент не от всех $m$ слагаемых, а только от небольшой выборки. В связи с большой размерностью $\boldsymbol w$ имеет смысл применить методы покомпонентных (блочных) градиентных спусков, что тоже является разновидностью стохастического спуска. Наличие регуляризации наталкивает на мысль использования проксимального метода. Конечно, для эффективной сходимости градиентный спуск должен быть ускоренным.

\section{Outcomes}

Предлагается экспериментально на модельных данных сравнить эффективность нескольких методов и выбрать наилучший. Сконструировать некоторую(ые) суперпозицию(ии) этих методов и проверить численным экспериментом эффективность суперпозиции(ий). Для этой суперпозиции сделать теоретические оценки на сходимость метода (попытаться перенести оценки базовых методов). Выходом проекта будет код реализации базовых методов и их суперпозиций, графики их сходимости, а так же теоретические оценки сходимости суперпозиции методов.

\section{Литературный обзор}

В работе \cite{Ogaltsov2020} предложено несколько методов ускорения адаптивного стохастического градиентного спуска, которые будет импользоваться как базовые способы решения задачи. В соответствии с \cite{Ogaltsov2020}, при выборе шага в обычном постоянном стохастическом градиентном методе выполняется по формуле
$$\boldsymbol x^{k+1}=\boldsymbol x^k-\frac{1}{2L_k}\nabla^r f(\boldsymbol x^k,\boldsymbol \xi^{k+1}),$$
где $L_k = L$~---~константа Липшица градиента, а $r$~---~размер банча (кол-во $f_k$, которые берутся для вычисления стохастического градиента). Этот метод сделается адаптивным, если подбирать на каждой итерации более точным способом $L_k$. Его так же можно ускорить использовав стандартную идею ускорения: использовать усреднение по градиентам за несколько последних шагов и смотреть на градиент не в текущей точке, а на перед. Конкретный метод ускорения берется из \cite{Ogaltsov2019}.

В \cite{Hendrikx2020} рассматривается способ решения задачи минимизации эмпирического риска ускоренным градиентным методом, посредством параллельного вычисления градиента. В \cite{Gower2019} доказывается теорема о сходимости бесконечного класса стохастических методов, определяемыми правилами выбора данных, используемых для формирования банчей, но не для ускоренных стохастических спусков. Общий подход, позволяющий ускорять почти произвольные неускоренные детерминированные и рандомизированные алгоритмы для гладких выпуклых задачах, предлагается в \cite{Ivanova2020} (на основе Каталиста). В \cite{Kamzolov2020} предлагаются супербыстрый метод второго порядка (неточный метод третьего порядка, использующие только производные второго порядка), в том числе рассматривается задача, близкая к нашей. Никакие методы второго порядка на прямую применить в нашей задаче не получиться в силу большой размерности, но, возможно, можно как-то адаптировать при использовании блочных стохастических методов. В \cite{Nesterov2010} \cite{Peter2011} \cite{Xiao2014} рассматриваются покомпонентные (блочные) стохастические спуски.

\section{Метрики качества}

В качестве метрик эффективности работы методов стохастического спуска будет использоваться: 

1) Минимальное значение функции $f(x_{end})$, до которого сошелся метод, пока скорость уменьшения значения функции не стала меньше некоторого заданного малого предела.

2) Минимальное значение отклонения $||x_{end}-x^*||_2$, до которого сошелся метод, пока скорость уменьшения значения функции не стал меньше некоторого заданного малого предела.

3) Скорость сходимости: $K=\sum\limits_{p=1}^{m} T_i$, где $T_p$~---~кол-во вызовов оракула для вычисления стохастического градиента (градиента функции $f_k$) на $p$ компонентах (если используется метод покоординатного спуска) до достижения нижнего предела скорости сходимости.
 
4) Теоретические оценки скорости сходимости метода в условиях поставленной задачи.


\section{Примерный план}
\begin{itemize}
	\item Реализовать адаптивный ускоренный непокомпонентный стохастический спуск, вкупе с проксимальным методом к 4 апреля.
	\item Потом разобраться в оболочке Каталист, реализовать/разобраться с покомпонентными (блочными) спусками и рассмотреть другие методы ускорений спуска.
	\item И составить некоторую суперпозицию этих методов - программа минимум.
	\item Сделать оценки на сходимость метода-суперпозиции  - если успею программу минимум.
	
\end{itemize}

\bibliographystyle{unsrt}
\bibliography{biblio2}

\end{document}











